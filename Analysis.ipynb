{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "from termcolor import colored, cprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def clean(text):\n",
    "    text = \"\".join([i.lower() for i in text if i not in string.punctuation])\n",
    "    text = re.sub(r\"([^-9A-Za-z ]|-)\", \" \" , text)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    words_new = [i for i in words if i not in stopwords]\n",
    "    text = \" \".join(words_new)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean data (or load cleaned data)\n",
    "\n",
    "**Warning:** if running for the first time, first run [this notebook](https://colab.research.google.com/github/Eyon42/Mergianos/blob/main/Analysis.ipynb#scrollTo=MZpuhKUUFlM7) to fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "\n",
    "if os.path.exists(\"data/clean_data.json\"):\n",
    "    with open(\"data/clean_data.json\") as f:\n",
    "        texts = json.load(f)\n",
    "else:\n",
    "    filenames = os.listdir(\"data/Aerothermodynamics\")\n",
    "    for filename in filenames:\n",
    "        with open(f\"data/Aerothermodynamics/{filename}\") as f:\n",
    "            text = f.read()\n",
    "            texts[filename.split(\".\")[0]] = clean(text)\n",
    "    with open(\"data/clean_data.json\", \"w\") as f:\n",
    "        json.dump(texts, f)\n",
    "        \n",
    "with open (\"data/Aerothermodynamics.json\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "model = tfidf.fit_transform(texts.values())\n",
    "\n",
    "words = list(tfidf.get_feature_names())\n",
    "documents = list(texts.keys())\n",
    "\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_word(word, nresults=None):\n",
    "    try: \n",
    "        wordId = words.index(word)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    results = tfidf_df.iloc[:,wordId].sort_values(ascending=False)\n",
    "    if nresults:\n",
    "        results = results.head(nresults)\n",
    "    return [(documents[i], s) for i, s in zip(results.index, results.values)]\n",
    "\n",
    "def search(query):\n",
    "    results = {}\n",
    "    query = clean(query)\n",
    "    for word in query.split(\" \"):\n",
    "        word_results = search_for_word(word)\n",
    "        for result in word_results:\n",
    "            try:\n",
    "                results[result[0]] = (results[result[0]] + result[1]) * 10\n",
    "            except KeyError:\n",
    "                results[result[0]] = result[1]\n",
    "\n",
    "    return results\n",
    "\n",
    "def display(data):\n",
    "    sortedData = {key: val for key, val in sorted(data.items(), key = lambda ele: ele[1], reverse=True)}\n",
    "    max_v = list(sortedData.values())[0]\n",
    "    for k,v in data.items():\n",
    "        print(f\"Document ID: {k} - Search score: {v/max_v}\\r\", end=\"\")\n",
    "        time.sleep(0.002)\n",
    "        if v == max_v:\n",
    "            print(colored(f\"Document ID: {k} - Search score: {v/max_v}{' '*10}\\r\", 'green', attrs=['reverse', 'blink']))\n",
    "            print(\"Title: \" + [i for i in metadata if i[\"id\"]==int(k)][0][\"title\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[7m\u001b[32mDocument ID: 20040161501 - Search score: 1.0          \n",
      "Title: Entry, Descent, and Landing: 2000-2004\n"
     ]
    }
   ],
   "source": [
    "r = search(\"aerothermodynamics in mars entry\")\n",
    "display(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "714f120e916982c9dc701e16014628e0450b217a4b9e82832580a5098815487e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
